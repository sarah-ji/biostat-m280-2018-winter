---
title: "Biostat M280 Homework 2"
subtitle: Due Feb 16 @ 11:59PM
output: html_document
---
  
```{r setup, include=FALSE}
#these will be the packages required to run this program. 
required_packages = c("tidyverse", "nycflights13", "ggstance", "lvplot", "knitr", "ggbeeswarm", "ggsci")

# this function will be checking first to see if the required packages are there or not then I'll install and load them at once.
ipak <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
}

# install and loading the required packages that are not aleady loaded
ipak(required_packages)

#hide my warnings and code for neatness
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

```{r}
library("tidyverse")
library("nycflights13")
library("ggstance")
library("lvplot")
library("knitr")
library("ggbeeswarm")
library("ggsci")
```

## Q1

Read [Chapter 7](http://r4ds.had.co.nz/exploratory-data-analysis.html) (Exploratory Data Analysis) of _R for Data Science_ and do exercises 7.3.4, 7.4.1, 7.5.1.1, 7.5.2.1, and 7.5.3.1.

### 7.3.4 Exercises
1. Explore the distribution of each of the x, y, and z variables in diamonds. What do you learn? Think about a diamond and how you might decide which dimension is the length, width, and depth.

```{r}
#histogram of the x variable
ggplot(data = diamonds) +
  geom_bar(mapping = aes(x = x))
```

```{r}
#histogram of the y variable
ggplot(data = diamonds) +
  coord_cartesian(xlim = c(0, 10)) +
  geom_bar(mapping = aes(x = y))
```

```{r}
#histogram of the z variable
ggplot(data = diamonds) +
  coord_cartesian(xlim = c(0, 10)) +
  geom_bar(mapping = aes(x = z))
```

```{r}
diamonds %>%
  mutate(id = row_number()) %>%
  select(x, y, z, id) %>%
  gather(variable, value, -id)  %>%
  ggplot(aes(x = value)) +
  geom_density() +
  geom_rug() +
  facet_grid(variable ~ .)
```

From the distributions of each of the x, y, and z variables in diamonds, we see that they are bi-modal and right skewed. As expected, there are higher counts of smaller diamonds than larger diamonds, and there are two outliers in y and z. 

I am not sure how to determine which of the dimensions x, y and z are, but from what we have read it seems that the variable x is length, variable y is width and variable z is the depth. This makes some intuitive sense, as most diamonds are not as deep as they are wide or long. Picturing a diamond on an engagement ring, most diamonds lay flatter to the finger than the width/length of the diamond. 

2. Explore the distribution of price. Do you discover anything unusual or surprising? (Hint: Carefully think about the binwidth and make sure you try a wide range of values.)

```{r}
ggplot(filter(diamonds), aes(x = price)) +
  geom_histogram(binwidth = 50, center = 0)
```

There are no diamonds with a price of $1,500.

```{r}
ggplot(filter(diamonds, price < 1600), aes(x = price, fill = carat)) +
  geom_histogram(binwidth = 10, center = 0)
```

```{r}
ggplot(filter(diamonds, price > 4000), aes(x = price, fill = carat)) +
  geom_histogram(binwidth = 10, center = 0)
```

3. How many diamonds are 0.99 carat? How many are 1 carat? What do you think is the cause of the difference?

```{r}
poin99carat <- diamonds %>% 
  filter(carat == 0.99)

n_99caret = dim(poin99carat)[1]

onecarat <- diamonds %>% 
  filter(carat == 1)

n_1caret = dim(onecarat)[1]

answer3 = c(n_99caret, n_1caret)
answer3 = as.data.frame(answer3)
rownames(answer3) = c("0.99 carat count", "1 carat count")
colnames(answer3) = "N"
answer3
```

There are a 23 diamonds with 0.99 carat size, and 1558 diamonds with 1 carat size. That is, there are more than 67 times as many 1 carat diamonds as 0.99 carat diamonds. I think the cause of the difference is that there are spikes at 0.25 carat intervals. It makes intuitive sense that the diamond producers are "rounding up" their diamond cuts, as there is a significant increase in price from a 0.99 carat size diamond to a 1.0 carat size diamond. 


```{r}
diamonds %>%
  filter(carat >= 0.9, carat <= 1.1) %>%
  count(carat) %>%
  print(n = 20)
```


```{r}
ggplot(data = diamonds, mapping = aes(x = carat)) +
  geom_histogram(binwidth = 0.01, aes(colour = cut_number(carat, 10)))
```

4. Compare and contrast coord_cartesian() vs xlim() or ylim() when zooming in on a histogram. What happens if you leave binwidth unset? What happens if you try and zoom so only half a bar shows?

coord_cartesian simply zooms in on the area specified by the limits. The calculation of the histogram is unaffected.

###7.4.1 Exercises
1. What happens to missing values in a histogram? What happens to missing values in a bar chart? Why is there a difference?

**Answer:** In a histogram, missing values are ignored, and only complete data is plotted. Conversely, in a bar chart, missing values are categorized as a separate category of values. 

2. What does na.rm = TRUE do in mean() and sum()?

```{r}
mean(c(1, 2, 3, NA), na.rm = TRUE)
```

```{r}
sum(c(1, 2, 3, NA), na.rm = TRUE)
```

**Answer:** na.rm = TRUE is an argument that can be specified within the mean() and sum() functions, to instruct the program to ignore missing values when computing the mean and sum.

###7.5.1.1 Exercises
1. Use what you’ve learned to improve the visualisation of the departure times of cancelled vs. non-cancelled flights.

```{r}
flights %>% 
  mutate(cancelled = is.na(dep_time) | is.na(arr_time)) %>% 
  ggplot() +
  geom_boxplot(mapping = aes(x = cancelled, y = dep_time))
```

2. What variable in the diamonds dataset is most important for predicting the price of a diamond? How is that variable correlated with cut? Why does the combination of those two relationships lead to lower quality diamonds being more expensive?

```{r}
quickndirty = lm(price ~ ., diamonds)
summary_quickndirty = summary(quickndirty)
kable(summary_quickndirty$coefficients, digits = 6)
```

**Answer:** We can do a quick and dirty check through doing a quick linear regression model to gain some sense of what predicts price best. It seems that carat and clarity are important. We will now use graphical methods to examine which covariate has the highest correlation with price.

```{r}
ggplot(data = diamonds, 
       mapping = aes(x = price,
                     colour = cut_number(carat, 10))) +
  geom_freqpoly()
```


```{r}
#seeing as how carat size obviously makes the most sense in price prediction,
#I want to check the distribution of carat size 
quantile(diamonds$carat)

#Let's categorize the top 10% of carat sizes as the jump from the 75th percentile to the max is huge!
c1 <- quantile(diamonds$carat, probs = .1) # bottom 10%
c2 <- quantile(diamonds$carat, probs = .5) #  median
c3 <- quantile(diamonds$carat, probs = .75)
c4 <- quantile(diamonds$carat, .9) # top 10%

```

```{r}
#taking a look at the different distributions for the categorized groups
diamonds %>% mutate(size_categories = cut(diamonds$carat, breaks = c(c1, c2, c3, c4))) %>%
    ggplot(aes(price)) +
    geom_density(aes(color = size_categories)) +
    ggsci::scale_color_d3(labels = c("bottom 10%", "median 50%" , "Above Average 75%", "top 10%"))
```


```{r}
ggplot(diamonds) +
  geom_point(aes(x = carat, y = price), color = "blue", alpha = 0.01)
```

**Answer** Volume and weight are two variables that is most important for predicting the price. Since volumn is highly correlated with weight, they can be considered to be one variable. It's hard to capture the ture relationship between cut and price, because cut and carat, and carat and price, are tightly related. We can get a sense of how the price deviates from just carat by computing the reidulas.

```{r}
library(modelr)
library(dplyr)
mod = lm(log(price) ~ log(carat), data = diamonds)

diamonds2 = diamonds %>% 
  add_residuals(mod)

ggplot(data = diamonds2) +
  geom_point(mapping = aes(x = carat, y = resid), color = "purple", alpha = 0.01)

```

Residuals are bell-shaped indicating that the most common carat size is most variable, which makes sense.

```{r}
ggplot(diamonds, aes(carat, price, color = clarity)) +
    geom_point() +
    stat_smooth(method = "lm") +
    facet_grid(~cut)
```


3. Install the ggstance package, and create a horizontal boxplot. How does this compare to using coord_flip()?

```{r}
ggplot(diamonds, aes(x = cut_number(price, 10), y = carat)) +
  geom_boxplot(aes(colour = cut_number(x,4))) +
  coord_flip() +
  xlab("Price")
```

```{r}
ggplot(diamonds) + geom_boxploth(aes(x = carat, y = cut_number(price, 10), colour = cut_number(x,4)))
```

**Answer** There appears to be no difference in using either or. I think it's quite useful to have the coor_flip() feature, actually.

4. One problem with boxplots is that they were developed in an era of much smaller datasets and tend to display a prohibitively large number of “outlying values”. One approach to remedy this problem is the letter value plot. Install the lvplot package, and try using geom_lv() to display the distribution of price vs cut. What do you learn? How do you interpret the plots?

```{r}
ggplot(diamonds, aes(x = cut, y = price, alpha = 0.0005)) +
  geom_lv(aes(colour = cut_number(carat, 10)))
```

5. Compare and contrast geom_violin() with a facetted geom_histogram(), or a coloured geom_freqpoly(). What are the pros and cons of each method?

```{r, warning= FALSE}
ggplot(diamonds) +
  geom_histogram(aes(x = price, colour = cut_width(price, width = 1500))) +
  facet_wrap(~cut)
```

```{r}
ggplot(diamonds) +
  geom_freqpoly(aes(x = price)) +
  facet_wrap(~cut)
```

```{r}
ggplot(diamonds) +
  geom_violin(aes(x = cut, y = price)) + coord_flip()
```


6. If you have a small dataset, it’s sometimes useful to use geom_jitter() to see the relationship between a continuous and categorical variable. The ggbeeswarm package provides a number of methods similar to geom_jitter(). List them and briefly describe what each one does.

```{r}

```

This is similar to geom_violin, in that it attempts show the density via width and similar to jitter in that overalpping points are dodged. The alternate function is geom_beeswarm() which just uses a slightly different positioning algorithim to re-located overlapping points

###7.5.2.1 Exercises
1. How could you rescale the count dataset above to more clearly show the distribution of cut within colour, or colour within cut?

```{r}

```


2. Use geom_tile() together with dplyr to explore how average flight delays vary by destination and month of year. What makes the plot difficult to read? How could you improve it?

```{r}

```


3. Why is it slightly better to use aes(x = color, y = cut) rather than aes(x = cut, y = color) in the example above?

```{r}

```


##7.5.3.1 Exercises
1. Instead of summarising the conditional distribution with a boxplot, you could use a frequency polygon. What do you need to consider when using cut_width() vs cut_number()? How does that impact a visualisation of the 2d distribution of carat and price?

```{r}

```

2. Visualise the distribution of carat, partitioned by price.

```{r}

```

3. How does the price distribution of very large diamonds compare to small diamonds. Is it as you expect, or does it surprise you?

```{r}

```

4. Combine two of the techniques you’ve learned to visualise the combined distribution of cut, carat, and price.

```{r}

```

5. Two dimensional plots reveal outliers that are not visible in one dimensional plots. For example, some points in the plot below have an unusual combination of x and y values, which makes the points outliers even though their x and y values appear normal when examined separately.

## Q3

Redo HW1 Q2 using tidyverse.

1. How many persons are in the data set (statisticians call this `n`)? How many SNPs are in the data set (statisticians call this `p`)?

```{r}
snpfam = read.delim(file = "/home/m280-data/hw1/merge-geno.fam", sep = " ", header = FALSE)
#number_of_people = dim(snpfam)[1]
number_of_people = n_distinct(snpfam$V2)
number_of_people
```

```{r}
snpbim = read_tsv(file = "/home/m280-data/hw1/merge-geno.bim", col_names = FALSE)
#number_of_snps = dim(snpbim)[1]
number_of_snps = n_distinct(snpbim$X2)
number_of_snps
```
**Answer:** There are n = 959 people in the dataset.
**Answer:** There are 8,348,674 SNP's in the dataset.

2. Which chromosomes does this data set contain? How many SNPs are in each chromosome?

```{r}
#chromosomes = unique(snpbim$V1)
#chromosomes
chromosomes <- snpbim %>% 
group_by(X1) %>% 
summarise(count = n())
chromosomes
#chromosome_counts = as.data.frame(table(snpbim$V1))
#names(chromosome_counts) = c("Chromosome", "# of SNP's")
#chromosome_counts
```

**Answer:** This dataset contains chromosomes 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21. Their counts are displayed below:

3. MAP4 (microtubule-associated protein 4) is a gene on chromosome 3 spanning positions 47,892,180 bp -- 48,130,769 bp. How many SNPs are located within MAP4 gene?

```{r}
#MAP4 = snpbim[(snpbim$V1 == 3 & 
#snpbim$V4 >= 47892180 & snpbim$V4 <= 48130769), ]
#numberofsnps_MAP4 = dim(MAP4)[1]
#numberofsnps_MAP4

numberofsnps_MAP4 <- snpbim %>% 
filter(X1 == 3 & X4 >= 47892180 & snpbim$X4 <= 48130769) %>%
summarise(count = n())
numberofsnps_MAP4
```

**Answer:** There are 894 SNP's located within the MAP4 gene.

4. Statistical geneticists often have to reformat a data set to feed into various analysis programs. For example, to use the Mendel software <http://www.genetics.ucla.edu/software/mendel>, we have to reformat the data set to be read by Mendel.

- Mendel's SNP definition file is similar to the plink `bim` file but has format  
`SNP ID`, `Chromosome`, `Base Pair Position`  
with each field separated by a comma. Write a Linux shell command to convert `merge-geno.bim` to Mendel SNP definition file.

```{r}
#snpdef_mendel = cbind(snpbim$V2, snpbim$V1, snpbim$V4)
#snpdef_mendel = as.factor(snpdef_mendel)
mendel_snp_def = snpbim %>% select(X2, X1, X4) %>% unite(format, sep =",")
#mendel_snp_def = as.data.frame(mendel_snp_def)
first_row = "    2.40 = FILE FORMAT VERSION NUMBER."
second_row = "8348674  = NUMBER OF SNPS LISTED HERE."
mendel_snp_def = rbind(first_row, second_row, mendel_snp_def)
kable(head(as.matrix(mendel_snp_def), 20))
```


- Mendel's pedigree file is similar to the plink `fam` file but has format  
`Family ID`, `Person ID`, `Father ID`, `Mother ID`, `Sex` coded as M or F, `Twin Status`  
with each field separated by a comma. Write a Linux shell command to convert `merge-geno.fam` to Mendel pedigree file. Since twin status is not available in plink format, we put nothing for that field. Also Mendel limits Person ID to have length less than or equal to 8 characters, so we have to strip the string `T2DG` from the IDs. 

```{r}
#get rid of T2DG in V2 of snpfam
snpfam$V2 = snpfam$V2 %>% str_replace("T2DG", "")
snpfam$V3 = snpfam$V3 %>% str_replace("T2DG", "")
snpfam$V4 = snpfam$V4 %>% str_replace("T2DG", "")
snpfam$V3 = na_if(snpfam$V3, 0) 
snpfam$V4 = na_if(snpfam$V4, 0) 
snpfam$V6 = na_if(snpfam$V6, 0) 
snpfam2 = replace_na(snpfam,
                     list(V1 = "", V2 = "",V3 = "", V4 = "", V5 = "", V6 = ""))
# if v3 or v4 0 then make them missing
snpfam2$V5 = str_replace_all(snpfam2$V5, c("1" = "M", "2" = "F"))
# make sex  12 
snpfam_final = snpfam2 %>% select(V1, V2, V3, V4, V5, V6) %>% unite(format, sep =",")
kable(head(snpfam_final, 20))
```

